{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b790991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from convokit import Corpus, download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "# df['messages'] = df['messages'].apply(lambda x: x[8:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09716f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a88639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d47f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = 'utterances.jsonl'\n",
    "\n",
    "with open(jsonl_file, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "flat_data = json_normalize(data)\n",
    "\n",
    "# Convert flattened JSON data to DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "columns_to_drop = [column for column in df.columns if column != 'text']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Save DataFrame to CSV file\n",
    "csv_file = 'provoking_file.csv'\n",
    "df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun_and_numbers(text):\n",
    "    # Combine punctuation and digits\n",
    "    text = str(text)\n",
    "    to_remove = string.punctuation + string.digits\n",
    "    text = text.lower()\n",
    "    return text.translate(str.maketrans('', '', to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a54313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    with open('model.pkl', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e95a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new column to the DataFrame\n",
    "def only_negatives(message):\n",
    "    result = model.predict([message])\n",
    "    if result == ['Negative']:\n",
    "        return message\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(only_negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.text))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d33709",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27371a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make rows 0,1,2,3 etc.\n",
    "df = df.reset_index(drop=True)\n",
    "len(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = len(df) // 2\n",
    "output_df = df.iloc[split_index:].copy()\n",
    "input_df = df.iloc[:split_index].copy()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc425e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'text' column to 'input' in input_df and to 'output' in output_df\n",
    "output_df.rename(columns={'text': 'output'}, inplace=True)\n",
    "input_df.rename(columns={'text': 'input'}, inplace=True)\n",
    "\n",
    "output_df.reset_index(drop=True, inplace=True)\n",
    "input_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate input_df and output_df vertically\n",
    "df = pd.concat([input_df, output_df], axis=1)\n",
    "df.output.apply(remove_pun_and_numbers)\n",
    "df.input.apply(remove_pun_and_numbers)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ce875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Concatenate input and output messages, separated by a special token\n",
    "eos_token = tokenizer.eos_token\n",
    "df[\"text\"] = df[\"input\"] + eos_token + df[\"output\"]\n",
    "\n",
    "# Save the processed dataset as a text file\n",
    "df[\"text\"].to_csv(\"processed_dataset.txt\", index=False, header=False)\n",
    "\n",
    "# Create a TextDataset and DataCollator\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=\"processed_dataset.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"chatbot_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    # changed device train batch size from 4 -> 1, less memory demanding\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=10_000,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
